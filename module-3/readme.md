# AI for Sound
## About
In this module, participants will learn about common sound patterns, how to collect and represent sound data, train models with them, generate new patterns with the trained models and get an overview of the common AI sound applications in art and other creative practices.

Graduates will:

    know sound from the physical perspective
    know sound from the neuroscientific perspective
    understand digital audio representations
    perform basic audi techniques for pre- and postprocessing
    Train Neural Networks for sound generation
    Use RNN or CNN for audio processing and computer listening
    sound based generative AI techniques

## Schedule
Program is work in progress

### 2025-10-13        Day 1 - Introduction to Sound
13:00 - 13:15     Check-in and introduction to Module 3 (Dr. Daniel Bisig)
13:15 - 14:00     Sound - The Physical Perspective (Sigve Haug, physicist)
14:00 - 14:45    Tutorial (Sigve)
14:45 - 15:15    Break
15:15 - 16:00    How do humans perceive and process sound (MSc. Riccardo Cusinato, neuroscientist)
16:00 - 16:45        Digital Representation of Sound  (Daniel) 
16:45 - 17:00        Check-out     

### 2025-10-14        Day 2 - Audio Perception and Analysis (Dr. Daniel Bisig)
13:00 - 13:15     Check-in
13:15 - 14:00        Computational Audio Feature Analysis (Daniel)
14:00 - 14:45     Experiments with Audio Clustering and Nearest Neighbors Search (Daniel)     
14:45 - 15:15    Break
15:15 - 16:00    Audio Vocoder and Audio Autoencoder (Daniel) 
16:00 - 16:45       Tutorial (Daniel) 
16:45 - 17:00       Check-out

### 2025-10-15        Day 3 - Generating Sound with AI 1 (Dr. Daniel Bisig)
13:00 - 13:15     Introduction to Huggingface
13:15 - 14:00        Working with Spaces on Huggingsface 
14:00 - 14:45     Tutorial        
14:45 - 15:15    Break
15:15 - 16:00     Introduction to AudioLDM
16:00 - 16:45        Tutorial 
16:45 - 17:00        Check-out

### 2025-10-16        Day 4 - Generating Sound with AI 2 (Dr. Giacomo Lepri)
13:00 - 13:15     Check-in
13:15 - 14:00        Artistic and Musical Applications of Neural Sound Synthesis
14:00 - 14:45     Introduction of RAVE    
14:45 - 15:15    Break
15:15 - 17:00     Stacco: a Digital Instrument for Performing with RAVE

### 2025-10-17        Day 5 - Generating Sound with AI 3 (Dr. Olivier Pasquet)
13:00 - 13:15     Check-in
13:15 - 14:00        Sound Morphing with Stable Diffusion
14:00 - 14:45     Tutorial        
14:45 - 15:15    Break
15:15 - 16:00     Hacking Text to Speech Synthesis
16:15 - 16:30        Planning module work (if you are not doing the essay).
16:30 - 16:45        Round up of Module 3
16:45 - 17:00        Check-out and feedback form
17:00 - 19:00     Apero (depends on on-site participation)

### "Sound - The Physical Perspective" - Lecture with PD Dr. Sigve Haug
What is the main phenomenmology of sound? Which concepts is physics using to account for this phenomenology? We will play with some sound tools to help our understanding of these concepts. 

"How do we perceive sound: perspectives from neuroscience" - Lecture with MSc. Riccardo Cusinato
How does our brain allow us to hear and interpret the sounds we encounter, from language to music? In this lecture, I will give you some answers, based on neuroscience. I will first outline the basics of auditory neuroscience and brain activity recordings. Then, I'll give some concrete examples of how we can study the perception of simple and complex sounds. Finally, I'll present how machine learning and artificial intelligence are helping neuroscientists in this endeavor.

### Lecture with Dr. Olivier Pasquet
I have been intensively composing music for a wide range of different formats of productions, including theatre, electronic, classical, contemporary, dance, installations, etc. Throughout these works, I have always sensed that there were sufficient personal common threads to weave together into a unique artistic style and aesthetic. It took me some time of exploration, successes, and failures, before discovering a distinctive and genuine direction that incorporates my language while still embracing the rich diversity of those formats.
      For instance, collaborating with theatre actors fosters exchanges of cultures and constructive ideas. However, music, an essential element in dramaturgy, must also be composed with consideration of live vocal interference. In contrast, the Hörspiel format, which utilizes recorded voices, allows for a different focus on the interplay between text, voice, sound, and music. By fusing these elements together, one can create not only a pure music piece but also a successful multimodal work. This artistic crucible, where various media and people converge, is what fascinates me about performance.
       Employing large language models and voice synthesis takes it a step further, as it can establish a continuous workflow from text to voice and music. All these elements can be merged together to create a composition where each component is not composed separately, but rather simultaneously, using parameters and constraints that cannot be clearly isolated or described. This approach is not new, but such “AI” systems represent an extension to again explore the beautiful question of "music without human performers".  It also brings me closer to the symbolic, artificial, and synthetic aspects I have always loved.
      Currently, AI systems may have become almost too realistic to remain artistically interesting. They also offer too little control to be fully satisfactory tools for the time being. Therefore, they deserve to be hacked to make them truly personalized, for someone like me who needs to personalize elements in his compositions.
      We will practice together with local Large Language Models a bit. Then, we will focus more on two hacked voice synthesis engines, yielding two distinct outcomes and ideas. The first one can officially “produce" multilingual speech and non-verbal communication, such as laughing, sighing, and crying. Pre-trained models are uncensored and ready for local inference. The second one, utilizing stable diffusion, enables sophisticated “morphing” between generated voices and other elements. I will then show how I implemented those in my workflow, from Python to Max, for two or three new pieces and a released pop music remix.
      We focus on music theatre in this presentation. But I am also exploring those sound syntheses for “pure" computer music. So, I will try to finish with a current work in progress for a future piece without voice, using upscaling and stable diffusion for generating sonograms from fabricated spectral delays.

### Lecture with Dr. Giacomo Lepri
Neural audio synthesis uses machine learning models to generate or transform sound, enabling the creation of novel textures that go beyond traditional synthesis techniques. This class introduces neural audio synthesis within the landscape of contemporary electronic music and sound art. First, we will discuss diverse art projects and applications that harness neural synthesis. Second, we will introduce RAVE: a novel framework for real-time audio generation. Students will experiment hands-on with pre-trained RAVE models to get a sense of their creative potential in both music and sound design. Additionally, students will have the opportunity to try Stacco, a newly designed instrument crafted specifically for expressive control of RAVE, aiming to bridge the gap between machine learning and embodied music performance.

## Assessments

https://drive.google.com/drive/folders/1SZ3zbEmVYxWbxixkB6qAPXr1ZHHqKfsr
